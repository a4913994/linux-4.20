                      =============
                      CFS Scheduler
                      =============


1.  OVERVIEW

CFS stands for "Completely Fair Scheduler," and is the new "desktop" process
scheduler implemented by Ingo Molnar and merged in Linux 2.6.23.  It is the
replacement for the previous vanilla scheduler's SCHED_OTHER interactivity
code.
CFS是“完全公平调度程序”的缩写，是由Ingo Molnar实现并合并到Linux 2.6.23的新的“桌面”进程调度程序。
它取代了以前普通调度程序的SCHED_OTHER互动性代码。

80% of CFS's design can be summed up in a single sentence: CFS basically models
an "ideal, precise multi-tasking CPU" on real hardware.
CFS 80%的设计可以用一句话概括：CFS在真实硬件上基本上模拟了一个“理想的、精确的多任务CPU”。

"Ideal multi-tasking CPU" is a (non-existent  :-)) CPU that has 100% physical
power and which can run each task at precise equal speed, in parallel, each at
1/nr_running speed.  For example: if there are 2 tasks running, then it runs
each at 50% physical power --- i.e., actually in parallel.
“理想的多任务CPU”是一个（不存在的:-)）CPU，它具有100%的物理功率，可以以精确相等的速度并行运行每个任务，
每个任务以1/nr_running的速度运行。例如：如果有两个任务运行，那么它会以50%的物理功率运行每个任务——即实际上是并行运行的。

On real hardware, we can run only a single task at once, so we have to
introduce the concept of "virtual runtime."  The virtual runtime of a task
specifies when its next timeslice would start execution on the ideal
multi-tasking CPU described above.  In practice, the virtual runtime of a task
is its actual runtime normalized to the total number of running tasks.
在真实的硬件上，我们一次只能运行一个任务，因此我们必须引入“虚拟运行时间”的概念。
任务的虚拟运行时间指定了它在上述“理想的多任务CPU”上下一个时间片开始执行的时间。
实际上，任务的虚拟运行时间是其实际运行时间与运行任务总数的标准化结果。



2.  FEW IMPLEMENTATION DETAILS
一些实现细节

In CFS the virtual runtime is expressed and tracked via the per-task
p->se.vruntime (nanosec-unit) value.  This way, it's possible to accurately
timestamp and measure the "expected CPU time" a task should have gotten.
在CFS中，虚拟运行时间通过每个任务的p->se.vruntime（纳秒单位）值来表示和跟踪。这样，就可以准确地时间戳和测量任务应该获得的“期望CPU时间”。

[ small detail: on "ideal" hardware, at any time all tasks would have the same
  p->se.vruntime value --- i.e., tasks would execute simultaneously and no task
  would ever get "out of balance" from the "ideal" share of CPU time.  ]
小细节：在“理想”硬件上，任何时候所有任务的p->se.vruntime值都将相同，即任务将同时执行，没有任务会从“理想”的CPU时间份额中失衡。

CFS's task picking logic is based on this p->se.vruntime value and it is thus
very simple: it always tries to run the task with the smallest p->se.vruntime
value (i.e., the task which executed least so far).  CFS always tries to split
up CPU time between runnable tasks as close to "ideal multitasking hardware" as
possible.
CFS的任务选择逻辑基于p->se.vruntime值，因此非常简单：它总是尝试运行具有最小p->se.vruntime值的任务（即迄今执行最少的任务）
CFS总是尽可能地在可运行任务之间划分CPU时间，以接近“理想的多任务硬件”。

Most of the rest of CFS's design just falls out of this really simple concept,
with a few add-on embellishments like nice levels, multiprocessing and various
algorithm variants to recognize sleepers.
CFS的大部分设计都源于这个非常简单的概念，只需添加一些附加装饰，如nice值、多处理和各种算法变体，就可以识别出睡眠进程。



3.  THE RBTREE
红黑树

CFS's design is quite radical: it does not use the old data structures for the
runqueues, but it uses a time-ordered rbtree to build a "timeline" of future
task execution, and thus has no "array switch" artifacts (by which both the
previous vanilla scheduler and RSDL/SD are affected).
CFS的设计非常激进：它不使用旧的数据结构来管理运行队列，而是使用了一棵有序红黑树来构建任务执行的“时间轴”，
因此不会有“数组切换”这个问题（这个问题影响了之前的经典调度器、RSDL和SD）。

CFS also maintains the rq->cfs.min_vruntime value, which is a monotonic
increasing value tracking the smallest vruntime among all tasks in the
runqueue.  The total amount of work done by the system is tracked using
min_vruntime; that value is used to place newly activated entities on the left
side of the tree as much as possible.
CFS还维护rq->cfs.min_vruntime值，该值是所有任务在运行队列中最小的vruntime值，并且它是单调递增的。
通过min_vruntime跟踪系统完成的总工作量，该值被用于尽可能地将新激活的实体放置在树的左侧。

The total number of running tasks in the runqueue is accounted through the
rq->cfs.load value, which is the sum of the weights of the tasks queued on the
runqueue.
运行队列中正在运行的任务总数通过rq->cfs.load值进行统计，该值是运行队列中排队的任务权重之和。

CFS maintains a time-ordered rbtree, where all runnable tasks are sorted by the
p->se.vruntime key. CFS picks the "leftmost" task from this tree and sticks to it.
As the system progresses forwards, the executed tasks are put into the tree
more and more to the right --- slowly but surely giving a chance for every task
to become the "leftmost task" and thus get on the CPU within a deterministic
amount of time.
CFS维护一个按时间顺序排列的RB树，其中所有可运行的任务按p->se.vruntime键进行排序。
CFS从该树中挑选最左边的任务并将其保留。随着系统的前进，执行的任务被放置到该树中越来越靠右的位置，
从而逐渐给每个任务一个成为“最左边的任务”的机会，并在确定的时间内获得CPU。

Summing up, CFS works like this: it runs a task a bit, and when the task
schedules (or a scheduler tick happens) the task's CPU usage is "accounted
for": the (small) time it just spent using the physical CPU is added to
p->se.vruntime.  Once p->se.vruntime gets high enough so that another task
becomes the "leftmost task" of the time-ordered rbtree it maintains (plus a
small amount of "granularity" distance relative to the leftmost task so that we
do not over-schedule tasks and trash the cache), then the new leftmost task is
picked and the current task is preempted.
总结一下，CFS的工作方式是：它运行一个任务一小段时间，当该任务进行调度（或发生调度器滴答声）时，
其CPU使用情况被“计算”: 刚刚使用实际CPU的（少量）时间被添加到p->se.vruntime中。一旦p->se.vruntime变得足够高，
以至于另一个任务成为维护的按时间顺序排列的RB树中的“最左边的任务”（加上与最左边的任务相对的一小段“粒度”距离，
以便我们不会过度调度任务和垃圾缓存），则选择新的最左边的任务，并抢占当前任务。



4.  SOME FEATURES OF CFS

CFS uses nanosecond granularity accounting and does not rely on any jiffies or
other HZ detail.  Thus the CFS scheduler has no notion of "timeslices" in the
way the previous scheduler had, and has no heuristics whatsoever.  There is
only one central tunable (you have to switch on CONFIG_SCHED_DEBUG):
CFS使用纳秒级粒度的记账，不依赖于任何jiffies或其他HZ的细节。
因此，CFS调度程序没有以前的调度程序中的“时间片”概念，也没有任何启发式方法。只有一个中央可调参数（您必须打开CONFIG_SCHED_DEBUG）：

   /proc/sys/kernel/sched_min_granularity_ns

which can be used to tune the scheduler from "desktop" (i.e., low latencies) to
"server" (i.e., good batching) workloads.  It defaults to a setting suitable
for desktop workloads.  SCHED_BATCH is handled by the CFS scheduler module too.
可用于将调度程序从“桌面”（即低延迟）调整到“服务器”（即良好的批处理）工作负载的设置。
它默认为适用于桌面工作负载的设置。CFS调度程序模块也处理SCHED_BATCH。

Due to its design, the CFS scheduler is not prone to any of the "attacks" that
exist today against the heuristics of the stock scheduler: fiftyp.c, thud.c,
chew.c, ring-test.c, massive_intr.c all work fine and do not impact
interactivity and produce the expected behavior.
由于其设计，CFS调度程序不容易受到当前对于原有调度程序启发式方法的任何“攻击”的影响：fiftyp.c， thud.c，chew.c ，ring-test.c，massive_intr.c 等都能正常工作且不影响交互性，并产生预期的行为。

The CFS scheduler has a much stronger handling of nice levels and SCHED_BATCH
than the previous vanilla scheduler: both types of workloads are isolated much
more aggressively.
与以往的普通调度程序相比，CFS调度程序在对nice值和SCHED_BATCH的处理上更加严格，对这两种类型的工作负载进行了更为有效的隔离。

SMP load-balancing has been reworked/sanitized: the runqueue-walking
assumptions are gone from the load-balancing code now, and iterators of the
scheduling modules are used.  The balancing code got quite a bit simpler as a
result.
SMP负载均衡机制已经得到重新设计/清理：现在，负载均衡代码中已经消除了对运行等待队列的假定，同时调度模块的迭代器被采用。由此，负载均衡代码的实现变得更加简单。



5. Scheduling policies

CFS implements three scheduling policies:
CFS实现了三种调度策略：

  - SCHED_NORMAL (traditionally called SCHED_OTHER): The scheduling
    policy that is used for regular tasks.
    SCHED_NORMAL（传统上称为SCHED_OTHER）：这是为普通任务使用的调度策略。

  - SCHED_BATCH: Does not preempt nearly as often as regular tasks
    would, thereby allowing tasks to run longer and make better use of
    caches but at the cost of interactivity. This is well suited for
    batch jobs.
    SCHED_BATCH：与正常任务相比，不会频繁抢占，因此使任务运行时间更长，更好地利用缓存，但代价是交互性较差。这非常适合批处理作业。

  - SCHED_IDLE: This is even weaker than nice 19, but its not a true
    idle timer scheduler in order to avoid to get into priority
    inversion problems which would deadlock the machine.
    SCHED_IDLE：这甚至比nice 19还要低，但它不是真正的空闲计时器调度程序，以避免陷入优先级反转问题，从而死锁了机器。

SCHED_FIFO/_RR are implemented in sched/rt.c and are as specified by
POSIX.

The command chrt from util-linux-ng 2.13.1.1 can set all of these except
SCHED_IDLE.



6.  SCHEDULING CLASSES

The new CFS scheduler has been designed in such a way to introduce "Scheduling
Classes," an extensible hierarchy of scheduler modules.  These modules
encapsulate scheduling policy details and are handled by the scheduler core
without the core code assuming too much about them.
新的CFS调度程序已经被设计成引入“调度类”，这是一个可扩展的调度程序模块层次结构。
这些模块封装了调度策略细节，并由调度程序核心处理，核心代码不会对它们做过多的假设。

sched/fair.c implements the CFS scheduler described above.
sched/fair.c实现了上述的CFS调度程序。

sched/rt.c implements SCHED_FIFO and SCHED_RR semantics, in a simpler way than
the previous vanilla scheduler did.  It uses 100 runqueues (for all 100 RT
priority levels, instead of 140 in the previous scheduler) and it needs no
expired array.
sched/rt.c实现了SCHED_FIFO和SCHED_RR语义的调度程序，相比之前的vanilla调度程序，它更简化了。
它使用100个运行队列（用于所有100个RT优先级级别，而之前的调度程序使用的是140个），并且不需要过期数组。

Scheduling classes are implemented through the sched_class structure, which
contains hooks to functions that must be called whenever an interesting event
occurs.
调度类是通过sched_class结构来实现的，其中包含了要调用的函数的钩子，这些函数会在发生有趣的事件时被调用。

This is the (partial) list of the hooks:

 - enqueue_task(...)

   Called when a task enters a runnable state.
   It puts the scheduling entity (task) into the red-black tree and
   increments the nr_running variable.
   当一个任务进入可运行状态时，会调用此函数。它将调度实体（任务）放入红黑树中并增加nr_running变量的值。

 - dequeue_task(...)

   When a task is no longer runnable, this function is called to keep the
   corresponding scheduling entity out of the red-black tree.  It decrements
   the nr_running variable.
   当任务不再可运行时，会调用此函数将相应的调度实体从红黑树中移除。它会递减nr_running变量的值。

 - yield_task(...)

   This function is basically just a dequeue followed by an enqueue, unless the
   compat_yield sysctl is turned on; in that case, it places the scheduling
   entity at the right-most end of the red-black tree.
   此函数基本上只是一个出队（dequeue）再入队（enqueue）的过程，除非启用了compat_yield sysctl，这种情况下，则将调度实体放在红黑树的最右端。

 - check_preempt_curr(...)

   This function checks if a task that entered the runnable state should
   preempt the currently running task.
   此函数用于检查进入可运行状态的任务是否应该抢占当前正在运行的任务。

 - pick_next_task(...)

   This function chooses the most appropriate task eligible to run next.
   此函数选择下一个最合适的可以运行的任务。

 - set_curr_task(...)

   This function is called when a task changes its scheduling class or changes
   its task group.
   当任务改变其调度类别或任务组时，将调用此函数。

 - task_tick(...)

   This function is mostly called from time tick functions; it might lead to
   process switch.  This drives the running preemption.
   这个函数大多数情况下是从时间滴答函数中被调用的；它可能会导致进程切换。这会驱动正在运行的抢占。




7.  GROUP SCHEDULER EXTENSIONS TO CFS
CFS的任务组调度扩展

Normally, the scheduler operates on individual tasks and strives to provide
fair CPU time to each task.  Sometimes, it may be desirable to group tasks and
provide fair CPU time to each such task group.  For example, it may be
desirable to first provide fair CPU time to each user on the system and then to
each task belonging to a user.
通常情况下，调度器是基于单个任务运行的，并努力为每个任务提供公平的CPU时间。
有时，将任务分组并为每个任务组提供公平的CPU时间可能是有益的。
例如，首先为系统上的每个用户提供公平的CPU时间，然后再为每个用户的任务提供公平的CPU时间，这可能是可取的。

CONFIG_CGROUP_SCHED strives to achieve exactly that.  It lets tasks to be
grouped and divides CPU time fairly among such groups.
CONFIG_CGROUP_SCHED正是为了实现这个目标而努力的。它允许将任务进行分组，并在这些组之间公平地划分CPU时间。

CONFIG_RT_GROUP_SCHED permits to group real-time (i.e., SCHED_FIFO and
SCHED_RR) tasks.
CONFIG_RT_GROUP_SCHED允许对实时任务（即SCHED_FIFO和SCHED_RR）进行分组。

CONFIG_FAIR_GROUP_SCHED permits to group CFS (i.e., SCHED_NORMAL and
SCHED_BATCH) tasks.
CONFIG_FAIR_GROUP_SCHED允许对CFS任务（即SCHED_NORMAL和SCHED_BATCH）进行分组。

   These options need CONFIG_CGROUPS to be defined, and let the administrator
   create arbitrary groups of tasks, using the "cgroup" pseudo filesystem.  See
   Documentation/cgroup-v1/cgroups.txt for more information about this filesystem.
   这些选项需要定义CONFIG_CGROUPS，并使用"cgroup"伪文件系统使管理员可以创建任意任务组。
   请参阅Documentation/cgroup-v1/cgroups.txt，了解有关该文件系统的更多信息。

When CONFIG_FAIR_GROUP_SCHED is defined, a "cpu.shares" file is created for each
group created using the pseudo filesystem.  See example steps below to create
task groups and modify their CPU share using the "cgroups" pseudo filesystem.
当定义CONFIG_FAIR_GROUP_SCHED时，对于使用伪文件系统创建的每个组，将创建一个“cpu.shares”文件。请参阅以下示例步骤，以使用“cgroups”伪文件系统创建任务组并修改它们的CPU份额。

	# mount -t tmpfs cgroup_root /sys/fs/cgroup
	# mkdir /sys/fs/cgroup/cpu
	# mount -t cgroup -ocpu none /sys/fs/cgroup/cpu
	# cd /sys/fs/cgroup/cpu

	# mkdir multimedia	# create "multimedia" group of tasks
	# mkdir browser		# create "browser" group of tasks

	# #Configure the multimedia group to receive twice the CPU bandwidth
	# #that of browser group

	# echo 2048 > multimedia/cpu.shares
	# echo 1024 > browser/cpu.shares

	# firefox &	# Launch firefox and move it to "browser" group
	# echo <firefox_pid> > browser/tasks

	# #Launch gmplayer (or your favourite movie player)
	# echo <movie_player_pid> > multimedia/tasks
